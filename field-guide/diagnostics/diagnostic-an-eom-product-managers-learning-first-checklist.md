---
description: A set of questions to reduce risk before increasing commitment
---

# Diagnostic: An EOM Product Manager’s Learning-First Checklist

This is not a delivery checklist.\
It is a **learning checklist** to use when work looks “obvious”, “known”, or “just delivery”.

Use it before planning, during planning, and whenever certainty suddenly feels high.

***

### 1. Assumptions & Certainty

* What are we assuming to be true that we have not yet validated?
* Which assumptions are we treating as facts?
* Which parts of this work feel “obvious” — and why?
* If this assumption were false, what would break first?
* Where are we relying on past experience from a different context?

**Signal to watch for:**\
People say “we already know this” without pointing to evidence from _this_ system.

***

### 2. Borrowed Confidence

* Where does our confidence actually come from?
* Is this confidence earned through evidence or borrowed from prior success?
* Are we assuming the new architecture will behave like the old one?
* Are integration partners being treated as predictable components?
* Who would be surprised if this turned out to be harder than expected?

**Signal to watch for:**\
Confidence is high, but no one can point to learning that occurred recently.

***

### 3. Evidence & Learning

* What would we need to learn to make a responsible commitment?
* What could we test quickly to reduce the biggest unknown?
* What learning would change our plan if it came out differently?
* Are we planning work before producing evidence?
* What would count as “invalidating” our current approach?

**Signal to watch for:**\
Learning is framed as optional or deferred “until after delivery”.

***

### 4. Risk-First Sequencing

* Which parts of the work carry the highest uncertainty?
* Are we sequencing work to maximise delivery or to maximise learning?
* What is the riskiest thing we could discover late?
* How could we surface that risk earlier?
* Which work would be waste if a key assumption proves false?

**Signal to watch for:**\
Low-risk work is front-loaded to show progress, while risky work is deferred.

***

### 5. Commitments & Language

* What are we being asked to commit to — and on what basis?
* Is this a hypothesis, an aspiration, or a true commitment?
* What evidence would make this commitment safer?
* Are we communicating uncertainty honestly or smoothing it over?
* What happens if we discover we’re wrong?

**Signal to watch for:**\
Commitments are phrased absolutely while learning is still incomplete.

***

### 6. Flow & Dependencies

* Where could we get blocked once work starts?
* Are dependencies being assumed or tested?
* Who controls the things we depend on?
* How quickly would we know if a dependency is failing us?
* Are we mapping dependencies — or exposing them?

**Signal to watch for:**\
Dependencies are described abstractly and owned by “someone else”.

***

### 7. Metrics & Signals

* Which metrics are we using as learning signals?
* Which metrics are being used to judge performance?
* Are any metrics tied to reward or reputation?
* What behaviours do these metrics encourage?
* What would people stop saying if this metric were removed?

**Signal to watch for:**\
Metrics are defended emotionally or treated as guarantees.

***

### 8. Stopping & Changing Direction

* Under what conditions would we stop or pivot?
* Have we said that out loud?
* Is stopping work seen as failure or competence?
* Would we recognise the signal to stop if it appeared?
* Does the system make stopping safe?

**Signal to watch for:**\
Stopping is never discussed, only finishing.

***

### 9. System Signals (Meta-Check)

* Are we being rewarded for learning — or for appearing certain?
* Does the environment make honest uncertainty risky?
* Are we designing for defensibility or for insight?
* What behaviour is the system currently making rational?
* What behaviour is it quietly discouraging?

**Signal to watch for:**\
You know the “right answer” to leadership before the evidence exists.

***

### Closing Prompt (Most Important Question)

> **What would we do differently if learning mattered more than being right?**

If the answer is “nothing”, the system is probably constraining learning — not the team.

***

#### How This Should Be Used

* As a _conversation guide_, not a form
* As a _diagnostic_, not a standard
* As a way to **protect learning before work accelerates**

For guidance on using this diagnostic safely in context, see:

* [Facilitation Guide: Using Diagnostics in Product Planning Conversations](../facilitation-guides/facilitation-guide-using-diagnostics-in-product-planning-conversations.md)

***

If PMs are punished for asking these questions, the problem is not PM skill.

It is the operating model.
