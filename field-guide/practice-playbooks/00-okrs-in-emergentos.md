# Playbook: OKRs (Outcome-Focused)

## Status

Illustrative practice guide\
Not mandatory. Not prescriptive. Not exhaustive.

***

## Purpose of OKRs in EmergentOS

In EmergentOS, OKRs exist to **make strategy testable through evidence**.

They are not:

* a performance management system
* a delivery contract
* a substitute for certainty
* a mechanism for compliance reporting

EOS uses OKRs primarily to:

* express strategic intent
* surface assumptions about value and feasibility
* anchor learning to measurable outcomes
* inform funding and governance decisions

**By default, OKRs in EmergentOS are aspirational.**

***

> **Lineage note**
>
> The “Who does what by how much” framing originates from _Who Does What by How Much_ by Jeff Gothelf and Josh Seiden, 2024.\
> EmergentOS adopts this structure because it clearly expresses intended outcome, ownership, and measurable change without prescribing solutions.
>
> The distinction between **aspirational outcome goals** and **commitments** is drawn from Marty Cagan’s Product Operating Model, as described in works such as _Inspired_, _Empowered_, and _Transformed_.
>
> Cagan emphasizes that empowered teams should operate primarily with outcome-focused aspirations, while acknowledging that modern enterprises occasionally require commitments due to regulatory, legal, or safety constraints.
>
> EmergentOS systematizes this distinction and adds an explicit requirement:
>
> **commitments must be earned through learning, not declared upfront.**
>
> [References](https://app.gitbook.com/s/oCIDNaq6Gp7xkyOJCltO/literature)

***

***

## The Problem OKRs Are Intended to Solve

Organizations struggle to connect:

* high-level intent
* day-to-day work
* evidence of impact

Common failure modes include:

* strategy that sounds meaningful but cannot be evaluated
* teams executing efficiently without knowing if value is created
* delivery success decoupled from customer outcomes
* governance relying on status rather than evidence

OKRs address this **only when they are treated as hypotheses**, not promises.

***

## Core Assumptions (Required for OKRs to Work)

OKRs are effective in EmergentOS _only if the following assumptions hold_:

1. Outcomes are uncertain at the outset
2. Learning will change priorities
3. Teams do not control results directly
4. Early indicators are imperfect but valuable
5. Revising goals based on learning is a success condition

If these assumptions are rejected, OKRs will degrade into theater.

***

## Two Modes of Intent in EmergentOS

EmergentOS explicitly supports **two different modes of intent**, each with different rules and governance behaviours:

1. **Aspirational OKRs** — the default mode
2. **High-Integrity Commitments** — the exceptional mode

Conflating these modes is the most common cause of OKR failure.

> Treating aspirations as commitments destroys learning.\
> Treating commitments as aspirations destroys trust.

***

## Aspirational OKRs (Default Mode)

Aspirational OKRs are used when outcomes are **uncertain** and learning is required.

### Characteristics

Aspirational OKRs:

* are hypothesis-driven
* describe desired outcomes, not promises
* are expected to evolve with learning
* are not fully controllable
* cannot be “failed” — only learned from

They answer the question:

> _If this strategy is sound, what change should we expect to observe?_

Missing an aspirational Key Result is **evidence**, not underperformance.

***

### Example: Aspirational Product OKR

**Objective**\
Improve first-time user success during onboarding.

**Key Results (Who / Does What / By How Much)**

* **New users** complete onboarding unaided **increasing from 45% to 65%**
* **Median time-to-first-value** reduced **from 3 days to 1 day**
* **Onboarding-related support requests** reduced **by 30%**

**Interpretation in EOS**

* These KRs express hypotheses about value
* Discovery shapes both solutions and measures
* KRs may change as learning accumulates
* Leadership evaluates evidence and insight, not scores

***

## High-Integrity Commitments (Exceptional Mode)

High-Integrity Commitments are used only when the organization must **guarantee an outcome** due to **external, non-negotiable constraints**.

Examples include:

* regulatory or compliance obligations
* legal deadlines
* safety, security, or ethical requirements
* contractual commitments

These situations reduce adaptability — but they do **not eliminate uncertainty**.

***

## High-Integrity Commitments: A Critical Constraint

In EmergentOS, **teams do not make High-Integrity Commitments without first performing sufficient learning** to justify confidence.

Even when regulations require delivery:

* feasibility is not assumed
* viability is not guaranteed
* risk must be surfaced early

A commitment made without evidence is **not a commitment** — it is a gamble.

***

### High-Integrity Commitments Defined

A **High-Integrity Commitment** is a high-value commitment that is:

* preceded by discovery or experimentation
* grounded in feasibility and viability evidence
* constrained deliberately to reduce risk
* transparent about remaining uncertainty

EmergentOS does not permit commitments that bypass learning.

***

### Example: Regulatory High-Integrity Commitment

**Context**\
A new regulation requires audited reporting by a fixed deadline.

**Pre-Commitment Learning**

* Technical feasibility spikes
* Data availability validation
* Compliance interpretation confirmed
* Delivery risks surfaced explicitly

**Commitment**\
Meet regulatory reporting requirements by 30 September.

**Outcome Conditions**

* Reports submitted in compliance with regulation X
* External audit completed with no material findings

**EOS Handling**

* Scope tightly constrained to regulatory need
* Discovery focuses on feasibility and risk mitigation
* Flow and quality are prioritised over experimentation
* Leadership explicitly acknowledges reduced adaptability

This commitment:

* is earned through learning
* does not cascade into aspirational team OKRs
* runs alongside, not in place of, outcome-driven OKRs

***

## Guardrails for High-Integrity Commitments

To prevent misuse, EmergentOS requires that commitments:

1. Are explicitly labelled as such
2. Include justification for determinism
3. Demonstrate prior learning or experimentation
4. Declare what adaptability is being sacrificed
5. Limit blast radius and duration
6. Are reviewed frequently for continued necessity
7. Revert to aspirational mode once the obligation is met

A proliferation of commitments is treated as a **systemic warning signal**, not planning maturity.

***

## How the Two Modes Coexist

In healthy EOS environments:

* Most work operates under aspirational OKRs
* Some work is constrained by High-Integrity Commitments
* Teams are never forced to pretend one is the other
* Governance behaviour adapts explicitly by mode

This allows organizations to:

* meet obligations credibly
* protect learning where uncertainty remains
* avoid reverting to blanket deterministic planning

***

## Anti-Patterns to Watch For

### Anti-Pattern: Aspirational OKRs Treated as Commitments

* Pressure to “hit numbers”
* Discovery suppressed
* Learning hidden

**Result:** Outcome theatre replaces strategy.

***

### Anti-Pattern: Commitments Without Learning

* Feasibility assumed
* Risk discovered late
* Teams blamed for systemic uncertainty

**Result:** Fragile delivery and loss of trust.

***

### Anti-Pattern: Everything Is a Commitment

* All work deemed “critical”
* No stopping or pivoting
* Chronic overload

**Result:** Determinism smuggled back in.

***

## Governance Implications

Governance behaviour shifts by intent mode:

* **Aspirational OKRs** → _What have we learned?_
* **High-Integrity Commitments** → _What risks remain and how are we mitigating them?_

Asking the wrong question in the wrong mode causes failure.

***

## Final Sanity Check

Before declaring an OKR or commitment, ask:

* Is this something we must guarantee, or something we must learn about?
* What evidence supports confidence here?
* What uncertainty still exists, and how is it mitigated?
* What would cause us to pause or de-scope?

If learning has not occurred, commitment is premature.

***

## EOS-Aligned Interpretation of Objectives

### What an Objective Is

An Objective is a **directional intent**, not a target state.

It:

* expresses _why_ the work matters
* frames the opportunity space
* remains stable long enough to support learning
* is understandable without knowing the solution

Good objectives are:

* outcome-oriented
* customer- or system-impact focused
* deliberately non-solution-specific

Bad objectives describe:

* features
* deliverables
* implementation detail
* internal activity

***

## EOS-Aligned Interpretation of Key Results

### What a Key Result Is

A Key Result is **evidence that an objective is being met**.

It:

* describes an observable change
* is used to test whether assumptions hold
* may evolve as learning increases
* is not guaranteed or controllable

EOS treats Key Results as:

* _signals of progress_, not commitments
* _learning tools_, not success criteria
* _inputs to decisions_, not scorecards

If a Key Result can be completed by effort alone, it is likely an output.

***

## How OKRs Interact with Discovery

In EmergentOS:

* Discovery generates candidate Key Results
* Key Results shape discovery focus
* Evidence from discovery refines or replaces Key Results

Discovery is not a prerequisite to OKRs.\
OKRs are a **reason to do discovery well**.

When discovery and OKRs are separated:

* OKRs become guesses
* discovery becomes disconnected
* learning loses strategic relevance

***

## How OKRs Interact with Flow

OKRs define _why_ work exists.\
Flow reveals _how_ work moves.

EOS uses flow signals to:

* contextualize OKR progress
* identify delivery constraints affecting outcomes
* detect when learning is delayed or blocked

Flow metrics never replace OKRs.\
OKRs never override flow signals.

Together, they prevent both:

* outcome theatre
* delivery theatre

***

## How OKRs Interact with Funding and Governance

In EmergentOS:

* OKRs inform funding decisions
* funding follows validated learning
* governance reviews evidence, not status

Governance questions shift from:

> “Are we on track?”

To:

> “What have we learned, and what does it justify next?”

Changing or stopping an OKR based on evidence is not failure. Persisting without evidence is.

***

## Common OKR Misuses (and Why They Occur)

### 1. OKRs as Task Lists

**Why it happens:**\
Discomfort with uncertainty.

**Result:**\
Outputs substitute for outcomes.

***

### 2. Frozen OKRs

**Why it happens:**\
Commitment culture and fear of reversal.

**Result:**\
Learning is suppressed to protect plans.

***

### 3. Cascaded OKRs

**Why it happens:**\
Desire for control and alignment through hierarchy.

**Result:**\
Local optimization and diluted intent.

***

### 4. OKRs Tied to Compensation

**Why it happens:**\
Misplaced belief that incentives drive outcomes.

**Result:**\
Gaming, risk aversion, metric manipulation.

***

### 5. Scoring as Success

**Why it happens:**\
Need for certainty and comparison.

**Result:**\
Focus shifts from learning to achievement optics.

***

## Signals of Healthy OKR Usage in EOS

OKRs are working when:

* Key Results are challenged and revised openly
* Teams discuss invalidated assumptions without blame
* Learning informs priority and funding changes
* Objectives remain stable while KRs evolve
* Failed KRs lead to insight, not defensiveness
* Leaders ask about evidence, not completion

***

## Signals of Harmful OKR Usage

OKRs are harmful when:

* teams feel pressure to hit numbers
* delivery metrics masquerade as outcomes
* OKRs are frozen despite contradictory evidence
* learning conversations disappear
* governance becomes status-driven again

When this occurs, the issue is not OKRs. It is a violation of EOS principles.

***

## Relationship to Other Practices

OKRs are most effective in EmergentOS when combined with:

* continuous discovery (to validate assumptions)
* adaptive funding (to respond to learning)
* flow diagnostics (to expose constraints)
* Lean Kata or improvement cycles (to evolve strategy)
* coaching-led governance (to interpret evidence)

None of these are mandatory — but isolation increases misuse risk.

***

## Concrete Examples: Good OKRs vs Anti-Patterns

Understanding OKRs conceptually is necessary but insufficient.\
Most OKR misuse occurs because teams lack **clear reference examples** for what “good” actually looks like in practice.

The following examples use the structure:

**Who → Does What → By How Much**

This framing makes ownership, intent, and evidence explicit—without prescribing solutions.

***

## Example 1: Product Discovery and Adoption

### EOS-Aligned OKR (Well-Structured)

**Objective**\
Improve first-time user success for the onboarding experience.

**Key Results**

* New users complete onboarding without assistance **increasing from 45% to 65%**
* Time-to-first-success event **reduced from 3 days to 1 day**
* User-reported onboarding friction **reduced by 30%**

**Why this works**

* _Who:_ New users
* _Does What:_ Successfully onboard and reach first value
* _By How Much:_ Explicit, measurable change
* Solution-neutral and discovery-friendly
* Encourages experimentation and learning

***

### Anti-Pattern: Output-Based OKRs

**Objective**\
Deliver the new onboarding flow.

**Key Results**

* Build onboarding screens
* Release onboarding v2
* Add product tours

**Why this fails**

* Measures activity, not outcomes
* No learning signal
* Completion guarantees success optics regardless of impact
* Encourages delivery over discovery

***

## Example 2: Platform Reliability

### EOS-Aligned OKR (Well-Structured)

**Objective**\
Improve system reliability during peak usage periods.

**Key Results**

* Customer-visible incidents **reduced from 5 per quarter to fewer than 2**
* Mean time to recovery **reduced from 90 minutes to under 30**
* Error-related support tickets **reduced by 40%**

**Why this works**

* _Who:_ Customers and support teams
* _Does What:_ Experience fewer and shorter incidents
* _By How Much:_ Clear thresholds
* Aligns engineering effort with customer impact
* Enables trade-offs between prevention, detection, and recovery

***

### Anti-Pattern: Metric-as-Target OKRs

**Objective**\
Improve DORA metrics.

**Key Results**

* Increase deployment frequency by 20%
* Reduce lead time by 30%
* Achieve 99.9% uptime

**Why this fails**

* Metrics become goals rather than signals
* Encourages gaming and risk avoidance
* Decouples engineering effort from user experience
* Violates EOS principle: _metrics inform learning, not compliance_

***

## Example 3: Flow and Decision Latency

### EOS-Aligned OKR (Well-Structured)

**Objective**\
Reduce delay caused by cross-team dependencies.

**Key Results**

* Average dependency-related wait time **reduced from 12 days to under 5**
* Number of unresolved cross-team blockers older than 7 days **reduced by 50%**
* Teams reporting blocked work more than once per sprint **reduced by 40%**

**Why this works**

* _Who:_ Delivery teams
* _Does What:_ Experience fewer and shorter blocking delays
* _By How Much:_ Observable system change
* Encourages structural improvement rather than local optimization

***

### Anti-Pattern: Activity-Centric Flow OKRs

**Objective**\
Improve delivery efficiency.

**Key Results**

* Implement Kanban across teams
* Reduce WIP limits
* Hold weekly flow reviews

**Why this fails**

* Confuses means with ends
* Implies tools guarantee outcomes
* Measures compliance, not effect
* Often leads to ritual adoption without impact

***

## Example 4: Learning and Validation

### EOS-Aligned OKR (Well-Structured)

**Objective**\
Increase confidence in investment decisions through faster learning.

**Key Results**

* Time to invalidate high-risk assumptions **reduced from 8 weeks to 3**
* Percentage of initiatives with validated hypotheses before scale **increased from 20% to 70%**
* Investments stopped or pivoted based on learning **increased quarter over quarter**

**Why this works**

* _Who:_ Leadership and delivery teams
* _Does What:_ Gain earlier, higher-quality learning
* _By How Much:_ Clear improvement signals
* Reinforces learning as success, not failure

***

### Anti-Pattern: Learning Theater

**Objective**\
Become more data-driven.

**Key Results**

* Run experiments
* Improve dashboards
* Share insights monthly

**Why this fails**

* No definition of impact
* Learning disconnected from decisions
* Easily satisfied without changing behavior
* Encourages reporting over adaptation

***

## Pattern Summary

Across all effective EOS-aligned OKRs:

* Objectives describe **intent**, not solutions
* Key Results describe **observable change**, not tasks
* Outcomes refer to **people, systems, or behavior**
* Numbers express **learning signals**, not commitments

Across all anti-patterns:

* Outputs replace outcomes
* Metrics become targets
* Completion substitutes for evidence
* Learning is implied but not required

***

## A Final Sanity Check

Before finalizing any OKR, ask:

* _If we hit every Key Result, what would actually be different?_
* _If we miss a Key Result, what would we learn?_
* _Could this OKR be “achieved” without improving outcomes?_

If the answers are unclear, the OKR is not ready.

***

## Closing Thought

Good OKRs make learning unavoidable.\
Bad OKRs make learning optional.

EmergentOS does not require perfection—it requires honesty.

OKRs are valuable only when they expose reality,\
even when that reality is uncomfortable.

OKRs are powerful because they are **aspirational**.\
Commitments are powerful because they are **earned**.

EmergentOS protects both by insisting that:

* learning precedes commitment
* determinism is explicit and constrained
* adaptability is consciously traded, not silently lost

Aspirations drive learning.\
High-integrity commitments sustain trust.
